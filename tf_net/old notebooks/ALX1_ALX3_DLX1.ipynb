{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "snpnet",
   "display_name": "snpnet",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn import preprocessing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1,out_channels=16, kernel_size=4,stride=1,padding=0)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=16,out_channels=64, kernel_size=2,stride=1,padding=0)\n",
    "        self.fc1 = nn.Linear(592, 320)\n",
    "        self.fc2 = nn.Linear(320, 160)\n",
    "        self.fc3 = nn.Linear(160, 80)\n",
    "        self.fc4 = nn.Linear(80, 2)\n",
    "        self.soft = nn.Softmax()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x.float()))\n",
    "        #x = F.relu(self.conv2(x))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        x = self.fc4(x)\n",
    "        #x = self.soft(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeqDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        sample = {'seq': self.data.iloc[idx][\"seq\"], 'label': self.data.iloc[idx][\"label\"]}\n",
    "        \n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def seq_to_tensor(seq):    \n",
    "    seq = list(seq[0])\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    le.fit(['A', 'T', 'C', 'G'])\n",
    "    test = le.transform(seq)\n",
    "    targets = torch.as_tensor(test,dtype=int)\n",
    "    targets = F.one_hot(targets, num_classes=4)\n",
    "    return targets.reshape(1,40,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data(tf):\n",
    "    pos_set = pd.read_csv(\"data/\"+tf+\".fa\",header=None).iloc[1::2]\n",
    "    pos_set = pos_set[pos_set[0].str.contains(\"N\")==False]\n",
    "    neg_set = pd.read_csv(\"data/\"+tf+\"_neg.fa\",header=None).iloc[1::2]\n",
    "    neg_set = neg_set[neg_set[0].str.contains(\"N\")==False]\n",
    "    pos_set[\"seq\"] = pos_set.apply(lambda x: seq_to_tensor(x), axis=1)\n",
    "    neg_set[\"seq\"] = neg_set.apply(lambda x: seq_to_tensor(x), axis=1)\n",
    "    pos_set[\"label\"] = torch.as_tensor(1)\n",
    "    neg_set[\"label\"] = torch.as_tensor(0)\n",
    "    data = pos_set.append(neg_set)\n",
    "    data.drop(columns=[0],inplace=True)\n",
    "    data = data.sample(frac=1)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test(data,batchsize=1,split=0.2):\n",
    "    batch_size = batchsize\n",
    "    trainset = SeqDataset(data.iloc[:int(len(data)*(1-split))])\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    testset = SeqDataset(data.iloc[int(len(data)*(1-split)):])\n",
    "    testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    return trainloader,testloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfs = [\"ALX1\",\"ALX3\",\"DLX1\"]\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[1,  2000] loss: 0.440\n",
      "[1,  4000] loss: 0.252\n",
      "[1,  6000] loss: 0.185\n",
      "[1,  8000] loss: 0.162\n",
      "[1, 10000] loss: 0.148\n",
      "[1, 12000] loss: 0.141\n",
      "[1, 14000] loss: 0.137\n",
      "[1, 16000] loss: 0.135\n",
      "[1, 18000] loss: 0.128\n",
      "[1, 20000] loss: 0.131\n",
      "[2,  2000] loss: 0.122\n",
      "[2,  4000] loss: 0.120\n",
      "[2,  6000] loss: 0.122\n",
      "[2,  8000] loss: 0.122\n",
      "[2, 10000] loss: 0.118\n",
      "[2, 12000] loss: 0.123\n",
      "[2, 14000] loss: 0.119\n",
      "[2, 16000] loss: 0.119\n",
      "[2, 18000] loss: 0.117\n",
      "[2, 20000] loss: 0.118\n",
      "[3,  2000] loss: 0.113\n",
      "[3,  4000] loss: 0.116\n",
      "[3,  6000] loss: 0.116\n",
      "[3,  8000] loss: 0.111\n",
      "[3, 10000] loss: 0.117\n",
      "[3, 12000] loss: 0.112\n",
      "[3, 14000] loss: 0.116\n",
      "[3, 16000] loss: 0.111\n",
      "[3, 18000] loss: 0.112\n",
      "[3, 20000] loss: 0.107\n",
      "[4,  2000] loss: 0.111\n",
      "[4,  4000] loss: 0.107\n",
      "[4,  6000] loss: 0.105\n",
      "[4,  8000] loss: 0.107\n",
      "[4, 10000] loss: 0.105\n",
      "[4, 12000] loss: 0.110\n",
      "[4, 14000] loss: 0.112\n",
      "[4, 16000] loss: 0.109\n",
      "[4, 18000] loss: 0.110\n",
      "[4, 20000] loss: 0.107\n",
      "[5,  2000] loss: 0.103\n",
      "[5,  4000] loss: 0.104\n",
      "[5,  6000] loss: 0.103\n",
      "[5,  8000] loss: 0.105\n",
      "[5, 10000] loss: 0.106\n",
      "[5, 12000] loss: 0.106\n",
      "[5, 14000] loss: 0.103\n",
      "[5, 16000] loss: 0.106\n",
      "[5, 18000] loss: 0.104\n",
      "[5, 20000] loss: 0.103\n",
      "[6,  2000] loss: 0.099\n",
      "[6,  4000] loss: 0.100\n",
      "[6,  6000] loss: 0.100\n",
      "[6,  8000] loss: 0.102\n",
      "[6, 10000] loss: 0.097\n",
      "[6, 12000] loss: 0.101\n",
      "[6, 14000] loss: 0.100\n",
      "[6, 16000] loss: 0.102\n",
      "[6, 18000] loss: 0.102\n",
      "[6, 20000] loss: 0.101\n",
      "[7,  2000] loss: 0.094\n",
      "[7,  4000] loss: 0.094\n",
      "[7,  6000] loss: 0.095\n",
      "[7,  8000] loss: 0.094\n",
      "[7, 10000] loss: 0.097\n",
      "[7, 12000] loss: 0.100\n",
      "[7, 14000] loss: 0.097\n",
      "[7, 16000] loss: 0.098\n",
      "[7, 18000] loss: 0.100\n",
      "[7, 20000] loss: 0.096\n",
      "[8,  2000] loss: 0.091\n",
      "[8,  4000] loss: 0.089\n",
      "[8,  6000] loss: 0.087\n",
      "[8,  8000] loss: 0.091\n",
      "[8, 10000] loss: 0.090\n",
      "[8, 12000] loss: 0.093\n",
      "[8, 14000] loss: 0.095\n",
      "[8, 16000] loss: 0.091\n",
      "[8, 18000] loss: 0.096\n",
      "[8, 20000] loss: 0.097\n",
      "[9,  2000] loss: 0.083\n",
      "[9,  4000] loss: 0.086\n",
      "[9,  6000] loss: 0.085\n",
      "[9,  8000] loss: 0.090\n",
      "[9, 10000] loss: 0.088\n",
      "[9, 12000] loss: 0.086\n",
      "[9, 14000] loss: 0.085\n",
      "[9, 16000] loss: 0.090\n",
      "[9, 18000] loss: 0.091\n",
      "[9, 20000] loss: 0.088\n",
      "[10,  2000] loss: 0.075\n",
      "[10,  4000] loss: 0.075\n",
      "[10,  6000] loss: 0.079\n",
      "[10,  8000] loss: 0.078\n",
      "[10, 10000] loss: 0.080\n",
      "[10, 12000] loss: 0.084\n",
      "[10, 14000] loss: 0.085\n",
      "[10, 16000] loss: 0.086\n",
      "[10, 18000] loss: 0.085\n",
      "[10, 20000] loss: 0.085\n",
      "Finished Training ALX1\n",
      "[1,  2000] loss: 0.631\n",
      "[1,  4000] loss: 0.469\n",
      "[1,  6000] loss: 0.343\n",
      "[1,  8000] loss: 0.300\n",
      "[1, 10000] loss: 0.282\n",
      "[1, 12000] loss: 0.278\n",
      "[1, 14000] loss: 0.269\n",
      "[1, 16000] loss: 0.258\n",
      "[1, 18000] loss: 0.262\n",
      "[1, 20000] loss: 0.260\n",
      "[1, 22000] loss: 0.254\n",
      "[1, 24000] loss: 0.258\n",
      "[1, 26000] loss: 0.251\n",
      "[1, 28000] loss: 0.250\n",
      "[1, 30000] loss: 0.246\n",
      "[1, 32000] loss: 0.242\n",
      "[1, 34000] loss: 0.243\n",
      "[1, 36000] loss: 0.242\n",
      "[2,  2000] loss: 0.238\n",
      "[2,  4000] loss: 0.231\n",
      "[2,  6000] loss: 0.237\n",
      "[2,  8000] loss: 0.232\n",
      "[2, 10000] loss: 0.230\n",
      "[2, 12000] loss: 0.225\n",
      "[2, 14000] loss: 0.230\n",
      "[2, 16000] loss: 0.231\n",
      "[2, 18000] loss: 0.224\n",
      "[2, 20000] loss: 0.228\n",
      "[2, 22000] loss: 0.223\n",
      "[2, 24000] loss: 0.226\n",
      "[2, 26000] loss: 0.224\n",
      "[2, 28000] loss: 0.227\n",
      "[2, 30000] loss: 0.222\n",
      "[2, 32000] loss: 0.230\n",
      "[2, 34000] loss: 0.225\n",
      "[2, 36000] loss: 0.227\n",
      "[3,  2000] loss: 0.209\n",
      "[3,  4000] loss: 0.215\n",
      "[3,  6000] loss: 0.219\n",
      "[3,  8000] loss: 0.219\n",
      "[3, 10000] loss: 0.211\n",
      "[3, 12000] loss: 0.216\n",
      "[3, 14000] loss: 0.217\n",
      "[3, 16000] loss: 0.216\n",
      "[3, 18000] loss: 0.221\n",
      "[3, 20000] loss: 0.220\n",
      "[3, 22000] loss: 0.219\n",
      "[3, 24000] loss: 0.216\n",
      "[3, 26000] loss: 0.213\n",
      "[3, 28000] loss: 0.216\n",
      "[3, 30000] loss: 0.213\n",
      "[3, 32000] loss: 0.217\n",
      "[3, 34000] loss: 0.215\n",
      "[3, 36000] loss: 0.215\n",
      "[4,  2000] loss: 0.204\n",
      "[4,  4000] loss: 0.212\n",
      "[4,  6000] loss: 0.205\n",
      "[4,  8000] loss: 0.206\n",
      "[4, 10000] loss: 0.209\n",
      "[4, 12000] loss: 0.208\n",
      "[4, 14000] loss: 0.207\n",
      "[4, 16000] loss: 0.212\n",
      "[4, 18000] loss: 0.207\n",
      "[4, 20000] loss: 0.210\n",
      "[4, 22000] loss: 0.210\n",
      "[4, 24000] loss: 0.211\n",
      "[4, 26000] loss: 0.206\n",
      "[4, 28000] loss: 0.211\n",
      "[4, 30000] loss: 0.212\n",
      "[4, 32000] loss: 0.205\n",
      "[4, 34000] loss: 0.201\n",
      "[4, 36000] loss: 0.212\n",
      "[5,  2000] loss: 0.199\n",
      "[5,  4000] loss: 0.199\n",
      "[5,  6000] loss: 0.198\n",
      "[5,  8000] loss: 0.200\n",
      "[5, 10000] loss: 0.201\n",
      "[5, 12000] loss: 0.203\n",
      "[5, 14000] loss: 0.203\n",
      "[5, 16000] loss: 0.203\n",
      "[5, 18000] loss: 0.202\n",
      "[5, 20000] loss: 0.203\n",
      "[5, 22000] loss: 0.195\n",
      "[5, 24000] loss: 0.205\n",
      "[5, 26000] loss: 0.203\n",
      "[5, 28000] loss: 0.205\n",
      "[5, 30000] loss: 0.203\n",
      "[5, 32000] loss: 0.201\n",
      "[5, 34000] loss: 0.203\n",
      "[5, 36000] loss: 0.211\n",
      "[6,  2000] loss: 0.189\n",
      "[6,  4000] loss: 0.196\n",
      "[6,  6000] loss: 0.194\n",
      "[6,  8000] loss: 0.192\n",
      "[6, 10000] loss: 0.191\n",
      "[6, 12000] loss: 0.200\n",
      "[6, 14000] loss: 0.190\n",
      "[6, 16000] loss: 0.194\n",
      "[6, 18000] loss: 0.196\n",
      "[6, 20000] loss: 0.196\n",
      "[6, 22000] loss: 0.197\n",
      "[6, 24000] loss: 0.199\n",
      "[6, 26000] loss: 0.198\n",
      "[6, 28000] loss: 0.197\n",
      "[6, 30000] loss: 0.200\n",
      "[6, 32000] loss: 0.199\n",
      "[6, 34000] loss: 0.198\n",
      "[6, 36000] loss: 0.202\n",
      "[7,  2000] loss: 0.186\n",
      "[7,  4000] loss: 0.185\n",
      "[7,  6000] loss: 0.190\n",
      "[7,  8000] loss: 0.186\n",
      "[7, 10000] loss: 0.188\n",
      "[7, 12000] loss: 0.186\n",
      "[7, 14000] loss: 0.192\n",
      "[7, 16000] loss: 0.189\n",
      "[7, 18000] loss: 0.185\n",
      "[7, 20000] loss: 0.195\n",
      "[7, 22000] loss: 0.189\n",
      "[7, 24000] loss: 0.193\n",
      "[7, 26000] loss: 0.196\n",
      "[7, 28000] loss: 0.190\n",
      "[7, 30000] loss: 0.195\n",
      "[7, 32000] loss: 0.193\n",
      "[7, 34000] loss: 0.192\n",
      "[7, 36000] loss: 0.197\n",
      "[8,  2000] loss: 0.177\n",
      "[8,  4000] loss: 0.182\n",
      "[8,  6000] loss: 0.179\n",
      "[8,  8000] loss: 0.180\n",
      "[8, 10000] loss: 0.179\n",
      "[8, 12000] loss: 0.189\n",
      "[8, 14000] loss: 0.186\n",
      "[8, 16000] loss: 0.181\n",
      "[8, 18000] loss: 0.185\n",
      "[8, 20000] loss: 0.186\n",
      "[8, 22000] loss: 0.185\n",
      "[8, 24000] loss: 0.186\n",
      "[8, 26000] loss: 0.185\n",
      "[8, 28000] loss: 0.192\n",
      "[8, 30000] loss: 0.192\n",
      "[8, 32000] loss: 0.185\n",
      "[8, 34000] loss: 0.187\n",
      "[8, 36000] loss: 0.187\n",
      "[9,  2000] loss: 0.170\n",
      "[9,  4000] loss: 0.171\n",
      "[9,  6000] loss: 0.179\n",
      "[9,  8000] loss: 0.172\n",
      "[9, 10000] loss: 0.176\n",
      "[9, 12000] loss: 0.173\n",
      "[9, 14000] loss: 0.178\n",
      "[9, 16000] loss: 0.177\n",
      "[9, 18000] loss: 0.179\n",
      "[9, 20000] loss: 0.181\n",
      "[9, 22000] loss: 0.182\n",
      "[9, 24000] loss: 0.181\n",
      "[9, 26000] loss: 0.183\n",
      "[9, 28000] loss: 0.181\n",
      "[9, 30000] loss: 0.182\n",
      "[9, 32000] loss: 0.179\n",
      "[9, 34000] loss: 0.178\n",
      "[9, 36000] loss: 0.185\n",
      "[10,  2000] loss: 0.164\n",
      "[10,  4000] loss: 0.163\n",
      "[10,  6000] loss: 0.171\n",
      "[10,  8000] loss: 0.168\n",
      "[10, 10000] loss: 0.170\n",
      "[10, 12000] loss: 0.167\n",
      "[10, 14000] loss: 0.176\n",
      "[10, 16000] loss: 0.174\n",
      "[10, 18000] loss: 0.174\n",
      "[10, 20000] loss: 0.172\n",
      "[10, 22000] loss: 0.174\n",
      "[10, 24000] loss: 0.173\n",
      "[10, 26000] loss: 0.176\n",
      "[10, 28000] loss: 0.177\n",
      "[10, 30000] loss: 0.175\n",
      "[10, 32000] loss: 0.176\n",
      "[10, 34000] loss: 0.174\n",
      "[10, 36000] loss: 0.171\n",
      "Finished Training ALX3\n",
      "[1,  2000] loss: 0.664\n",
      "[1,  4000] loss: 0.624\n",
      "[1,  6000] loss: 0.522\n",
      "[1,  8000] loss: 0.396\n",
      "[1, 10000] loss: 0.356\n",
      "[1, 12000] loss: 0.333\n",
      "[1, 14000] loss: 0.320\n",
      "[1, 16000] loss: 0.316\n",
      "[1, 18000] loss: 0.307\n",
      "[1, 20000] loss: 0.302\n",
      "[1, 22000] loss: 0.300\n",
      "[2,  2000] loss: 0.289\n",
      "[2,  4000] loss: 0.288\n",
      "[2,  6000] loss: 0.284\n",
      "[2,  8000] loss: 0.286\n",
      "[2, 10000] loss: 0.282\n",
      "[2, 12000] loss: 0.281\n",
      "[2, 14000] loss: 0.281\n",
      "[2, 16000] loss: 0.275\n",
      "[2, 18000] loss: 0.284\n",
      "[2, 20000] loss: 0.276\n",
      "[2, 22000] loss: 0.273\n",
      "[3,  2000] loss: 0.269\n",
      "[3,  4000] loss: 0.266\n",
      "[3,  6000] loss: 0.265\n",
      "[3,  8000] loss: 0.265\n",
      "[3, 10000] loss: 0.267\n",
      "[3, 12000] loss: 0.265\n",
      "[3, 14000] loss: 0.261\n",
      "[3, 16000] loss: 0.268\n",
      "[3, 18000] loss: 0.262\n",
      "[3, 20000] loss: 0.265\n",
      "[3, 22000] loss: 0.260\n",
      "[4,  2000] loss: 0.249\n",
      "[4,  4000] loss: 0.251\n",
      "[4,  6000] loss: 0.252\n",
      "[4,  8000] loss: 0.249\n",
      "[4, 10000] loss: 0.249\n",
      "[4, 12000] loss: 0.253\n",
      "[4, 14000] loss: 0.259\n",
      "[4, 16000] loss: 0.257\n",
      "[4, 18000] loss: 0.255\n",
      "[4, 20000] loss: 0.251\n",
      "[4, 22000] loss: 0.251\n",
      "[5,  2000] loss: 0.237\n",
      "[5,  4000] loss: 0.239\n",
      "[5,  6000] loss: 0.241\n",
      "[5,  8000] loss: 0.244\n",
      "[5, 10000] loss: 0.243\n",
      "[5, 12000] loss: 0.240\n",
      "[5, 14000] loss: 0.244\n",
      "[5, 16000] loss: 0.246\n",
      "[5, 18000] loss: 0.244\n",
      "[5, 20000] loss: 0.243\n",
      "[5, 22000] loss: 0.244\n",
      "[6,  2000] loss: 0.226\n",
      "[6,  4000] loss: 0.224\n",
      "[6,  6000] loss: 0.228\n",
      "[6,  8000] loss: 0.238\n",
      "[6, 10000] loss: 0.234\n",
      "[6, 12000] loss: 0.238\n",
      "[6, 14000] loss: 0.230\n",
      "[6, 16000] loss: 0.235\n",
      "[6, 18000] loss: 0.236\n",
      "[6, 20000] loss: 0.237\n",
      "[6, 22000] loss: 0.233\n",
      "[7,  2000] loss: 0.216\n",
      "[7,  4000] loss: 0.218\n",
      "[7,  6000] loss: 0.219\n",
      "[7,  8000] loss: 0.217\n",
      "[7, 10000] loss: 0.225\n",
      "[7, 12000] loss: 0.222\n",
      "[7, 14000] loss: 0.225\n",
      "[7, 16000] loss: 0.225\n",
      "[7, 18000] loss: 0.230\n",
      "[7, 20000] loss: 0.224\n",
      "[7, 22000] loss: 0.228\n",
      "[8,  2000] loss: 0.206\n",
      "[8,  4000] loss: 0.208\n",
      "[8,  6000] loss: 0.208\n",
      "[8,  8000] loss: 0.208\n",
      "[8, 10000] loss: 0.214\n",
      "[8, 12000] loss: 0.217\n",
      "[8, 14000] loss: 0.215\n",
      "[8, 16000] loss: 0.217\n",
      "[8, 18000] loss: 0.220\n",
      "[8, 20000] loss: 0.216\n",
      "[8, 22000] loss: 0.220\n",
      "[9,  2000] loss: 0.186\n",
      "[9,  4000] loss: 0.193\n",
      "[9,  6000] loss: 0.196\n",
      "[9,  8000] loss: 0.200\n",
      "[9, 10000] loss: 0.207\n",
      "[9, 12000] loss: 0.206\n",
      "[9, 14000] loss: 0.205\n",
      "[9, 16000] loss: 0.206\n",
      "[9, 18000] loss: 0.207\n",
      "[9, 20000] loss: 0.212\n",
      "[9, 22000] loss: 0.215\n",
      "[10,  2000] loss: 0.183\n",
      "[10,  4000] loss: 0.182\n",
      "[10,  6000] loss: 0.184\n",
      "[10,  8000] loss: 0.189\n",
      "[10, 10000] loss: 0.194\n",
      "[10, 12000] loss: 0.192\n",
      "[10, 14000] loss: 0.193\n",
      "[10, 16000] loss: 0.198\n",
      "[10, 18000] loss: 0.201\n",
      "[10, 20000] loss: 0.200\n",
      "[10, 22000] loss: 0.200\n",
      "Finished Training DLX1\n"
     ]
    }
   ],
   "source": [
    "for tf in tfs:\n",
    "    data = create_data(tf)\n",
    "    trainloader,testloader = get_train_test(data,batchsize=16,split=0.2)\n",
    "\n",
    "    net = Net()\n",
    "    net.to(device)\n",
    "        \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "    for epoch in range(10):  # loop over the dataset multiple times\n",
    "\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            input_key,label_key = data\n",
    "            inputs = data[input_key]\n",
    "            labels = data[label_key]\n",
    "\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "                print('[%d, %5d] loss: %.3f' %\n",
    "                    (epoch + 1, i + 1, running_loss / 2000))\n",
    "                running_loss = 0.0\n",
    "\n",
    "    print('Finished Training', tf)\n",
    "    torch.save(net.state_dict(), './models/'+tf+'.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ALX1\n",
      "Accuracy of the network on the 10000 test images: 96 %\n",
      "Accuracy for class 0.000000 is: 96.8 %\n",
      "Accuracy for class 1.000000 is: 96.7 %\n",
      "ALX3\n",
      "Accuracy of the network on the 10000 test images: 94 %\n",
      "Accuracy for class 0.000000 is: 91.7 %\n",
      "Accuracy for class 1.000000 is: 95.5 %\n",
      "DLX1\n",
      "Accuracy of the network on the 10000 test images: 92 %\n",
      "Accuracy for class 0.000000 is: 93.0 %\n",
      "Accuracy for class 1.000000 is: 92.7 %\n"
     ]
    }
   ],
   "source": [
    "for tf in tfs:\n",
    "    print(tf)\n",
    "    data = create_data(tf)\n",
    "    trainloader,testloader = get_train_test(data,batchsize=16,split=0.2)\n",
    "\n",
    "    net = Net()\n",
    "    net.load_state_dict(torch.load('./models/'+tf+'.pth'))\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    classes = [0,1]\n",
    "    correct_pred = {classname: 0 for classname in classes}\n",
    "    total_pred = {classname: 0 for classname in classes}\n",
    "\n",
    "    # since we're not training, we don't need to calculate the gradients for our outputs\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            input_key,label_key = data\n",
    "            inputs = data[input_key]\n",
    "            labels = data[label_key]\n",
    "            # calculate outputs by running images through the network\n",
    "            outputs = net(inputs)\n",
    "            # the class with the highest energy is what we choose as prediction\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            for label, prediction in zip(labels, predicted):\n",
    "                if label == prediction:\n",
    "                    correct_pred[classes[label]] += 1\n",
    "                total_pred[classes[label]] += 1\n",
    "\n",
    "    print('Accuracy of the network on the 10000 test images: %d %%' % (100 * correct / total))\n",
    "\n",
    "    for classname, correct_count in correct_pred.items():\n",
    "        accuracy = 100 * float(correct_count) / total_pred[classname]\n",
    "        print(\"Accuracy for class {:5f} is: {:.1f} %\".format(classname, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}